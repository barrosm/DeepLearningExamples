{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Representation (Parte 3).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "jUvKpGvh1yJN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table style=\"width:100%\">\n",
        "  <tr>\n",
        "    <td><center style=\"font-size:300%;\">Exemplo Prático (Emojify)</center></td>\n",
        "    <td><img src=\"https://logodownload.org/wp-content/uploads/2015/02/puc-rio-logo.gif\" width=\"100\"/></td> \n",
        "  </tr>    \n",
        "</table>\n",
        "\n",
        "Msc. Cristian Muñoz V."
      ]
    },
    {
      "metadata": {
        "id": "CwT-Ds1S27DP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "rm -rf glove.6B.50d.zip glove.6B.50d.txt emo_utils.py train_emoji.csv test.csv\n",
        "wget -O glove.6B.50d.zip https://github.com/crismunoz/DeepLearningExamples/raw/master/week2/resources/glove.6B.50d.zip\n",
        "wget -O emo_utils.py https://raw.githubusercontent.com/crismunoz/DeepLearningExamples/master/week2/emo_utils.py\n",
        "wget -O train_emoji.csv https://raw.githubusercontent.com/crismunoz/DeepLearningExamples/master/week2/train_emoji.csv\n",
        "wget -O test.csv https://raw.githubusercontent.com/crismunoz/DeepLearningExamples/master/week2/test.csv\n",
        "unzip glove.6B.50d.zip\n",
        "pip install emoji"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZfVcDcZ51dgs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from emo_utils import *\n",
        "import emoji\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xI8w06Bp4TFy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, Y_train = read_csv('train_emoji.csv')\n",
        "X_test, Y_test = read_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rAarw3iQ6B-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train[1], Y_train[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gvRWsaZH5Hur",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "maxLen = len(max(X_train, key=len).split())\n",
        "print(maxLen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iz3_J8XY50eE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "index = 10\n",
        "print(X_train[index], label_to_emoji(Y_train[index]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6lsPeNFK52U7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
        "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WvRjiafI5-3V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "index = 40\n",
        "print(Y_train[index], \"agora vai ser em one-hot encoding:\", Y_oh_train[index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BNHT-RIU6Gy8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_name='glove.6B.50d.txt'\n",
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7zTWvbnK6Jlk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word = \"homem\"\n",
        "index = 89846\n",
        "print(\"O índice de\", word, \"no vocabulario de palavras é: \", word_to_index[word])\n",
        "print(\"A palavra no index \", str(index) + \" do vocabulario é: '{}'\".format(index_to_word[index]))\n",
        "print (word_to_vec_map[word])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EtB-fbo77YP4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Definição do Modelo"
      ]
    },
    {
      "metadata": {
        "id": "zsZXkeVZ6Rf8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.initializers import glorot_uniform\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9paGTCtw6nOd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sentences_to_indices(X, word_to_index, max_len):   \n",
        "    total_examples = X.shape[0]\n",
        "    X_indices = np.zeros((len(X), max_len))\n",
        "    \n",
        "    for i in range(total_examples):\n",
        "        sentence_words = X[i].lower().split()\n",
        "        j = 0\n",
        "        for w in sentence_words:\n",
        "            X_indices[i, j] = word_to_index[w]\n",
        "            j = j + 1\n",
        "    \n",
        "    return X_indices\n",
        "  \n",
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    \n",
        "    vocab_len = len(word_to_index)+1                  # keras+1\n",
        "\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # Word embedding\n",
        "    \n",
        "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
        "    \n",
        "    for word, index in word_to_index.items():\n",
        "        emb_matrix[index, :] = word_to_vec_map[word]\n",
        "\n",
        "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)\n",
        "\n",
        "    embedding_layer.build((None,))\n",
        "    \n",
        "    embedding_layer.set_weights([emb_matrix])\n",
        "    \n",
        "    return embedding_layer\n",
        "  \n",
        "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E46Ectty8Axt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
        "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ALfVvxZT-vLH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Modelo Simple"
      ]
    },
    {
      "metadata": {
        "id": "v1rJESYh6vAu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Definir o modelo \"model\"\n",
        "import keras.backend as K\n",
        "from keras.layers import Lambda\n",
        "\n",
        "# y = camada()(X) onde X=entrada\n",
        "inp = Input(shape=(maxLen,))\n",
        "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "embedding = embedding_layer(inp)\n",
        "avg = Lambda(lambda x:K.mean(x, axis=1))(embedding)   # K chama o backend, no caso Tensorflow\n",
        "logits = Dense(5)(avg)\n",
        "output = Activation('softmax')(logits)\n",
        "\n",
        "model = Model(inputs=inp, outputs=output)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e1XYU9SV6zZt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train_indices, Y_train_oh, epochs = 100, batch_size = 32, shuffle=True)\n",
        "##validation_split=0.2, \n",
        "##initial_epoch=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dS4PYXCu63F1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
        "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
        "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
        "print()\n",
        "print(\"Accuracy Teste = \", acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ybCkYuBq65aN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
        "pred = model.predict(X_test_indices)\n",
        "for i in range(len(X_test)):\n",
        "    x = X_test_indices\n",
        "    num = np.argmax(pred[i])\n",
        "    if(num != Y_test[i]):\n",
        "        print('Correto emoji:'+ label_to_emoji(Y_test[i]) + ' previsão: '+ X_test[i] + label_to_emoji(num).strip())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-X82lNyy668F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_test = np.array(['not feeling happy'])\n",
        "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
        "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ii2tpwDp-knI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Modelo LSTM"
      ]
    },
    {
      "metadata": {
        "id": "HBdiDWep68at",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Definir o modelo \"model\"\n",
        "import keras.backend as K\n",
        "from keras.layers import Lambda\n",
        "hidden_size=128\n",
        "\n",
        "inp = Input(shape=(maxLen,))\n",
        "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "embedding = embedding_layer(inp)\n",
        "x = LSTM(hidden_size, return_sequences=False)(embedding)\n",
        "#x = Flatten()(embedding)\n",
        "#x = Dense(128)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "logits = Dense(5)(x)\n",
        "output = Activation('softmax')(logits)\n",
        "\n",
        "model = Model(inputs=inp, outputs=output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-kNVX9K269wt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Chamar o modelo\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aXI3-7FL6_q1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
        "Y_train_oh = convert_to_one_hot(Y_train, C = 5)\n",
        "\n",
        "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)\n",
        "##validation_split=0.2, \n",
        "##initial_epoch=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-beMyGpy7A_F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
        "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
        "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
        "print()\n",
        "print(\"Accuracy Teste = \", acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ckZGiXN87Ec9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
        "pred = model.predict(X_test_indices)\n",
        "for i in range(len(X_test)):\n",
        "    x = X_test_indices\n",
        "    num = np.argmax(pred[i])\n",
        "    if(num != Y_test[i]):\n",
        "        print('Correto emoji:'+ label_to_emoji(Y_test[i]) + ' previsão: '+ X_test[i] + label_to_emoji(num).strip())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ine_Vjai7L71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_test = np.array(['feeling happy'])\n",
        "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
        "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "convu1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barrosm/DeepLearningExamples/blob/master/convu1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ovhUa4YbIeL",
        "colab_type": "text"
      },
      "source": [
        "# Exemplo de rede neural convolucional\n",
        "\n",
        "Crie uma rede neural convolucional com o TensorFlow.\n",
        "\n",
        "Este exemplo está usando a API de camadas TensorFlow, consulte o exemplo 'convolutional_network_raw'\n",
        "para uma implementação bruta do TensorFlow com variáveis.\n",
        "\n",
        "\n",
        "- Project: https://github.com/aymericdamien/TensorFlow-Examples/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL5I9c_WbIeU",
        "colab_type": "text"
      },
      "source": [
        "## CNN Visão geral \n",
        "\n",
        "![CNN](http://personal.ie.cuhk.edu.hk/~ccloy/project_target_code/images/fig3.png)\n",
        "\n",
        "## Visão geral do conjunto de dados MNIST\n",
        "\n",
        "Este exemplo está usando dígitos manuscritos MNIST. O conjunto de dados contém 60.000 exemplos para treinamento e 10.000 exemplos para teste. Os dígitos foram normalizados em tamanho e centralizados em uma imagem de tamanho fixo (28x28 pixels) com valores de 0 a 1. Para simplificar, cada imagem foi achatada e convertida em uma matriz numérica 1-D de 784 recursos (28 * 28 )\n",
        "\n",
        "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
        "\n",
        "More info: http://yann.lecun.com/exdb/mnist/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSbsVb1ObIeX",
        "colab_type": "code",
        "outputId": "e7f535bd-63cb-42c6-95f0-19fa84ba7eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "%tensorflow_version 1.x    # o default é tensorflow 2, então tem que dizer que está trabalhando com o 1\n",
        "\n",
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "# Importar dados da base MNIST\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.x    # o default é tensorflow 2, então tem que dizer que está trabalhando com o 1`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:From <ipython-input-4-b98902509fc5>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsiEarVobIek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importar parametros de treinamento\n",
        "learning_rate = 0.001\n",
        "num_steps = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 784 # MNIST data input (img shape: 28*28)\n",
        "num_classes = 10 # MNIST total classes (0-9 digits)\n",
        "dropout = 0.25 # Dropout, probability to drop a unit  - probabilidade de 25% de um neuronio ser desligado no treinamento"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "Nb7j8SRAbIer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criando a rede neural\n",
        "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
        "    \n",
        "    # Definir um escopo para reutilizar as variáveis\n",
        "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
        "        # TF Estimator input is a dict, in case of multiple inputs\n",
        "        x = x_dict['images']\n",
        "\n",
        "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
        "        # Reshape to match picture format [Height x Width x Channel]\n",
        "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
        "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "        # Convolution Layer with 32 filters and a kernel size of 5\n",
        "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
        "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
        "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
        "\n",
        "        # Convolution Layer with 64 filters and a kernel size of 3\n",
        "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
        "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
        "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
        "\n",
        "        # Flatten the data to a 1-D vector for the fully connected layer\n",
        "        fc1 = tf.contrib.layers.flatten(conv2)\n",
        "\n",
        "        # Fully connected layer (in tf contrib folder for now)\n",
        "        fc1 = tf.layers.dense(fc1, 1024)\n",
        "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
        "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
        "\n",
        "        # Output layer, class prediction\n",
        "        out = tf.layers.dense(fc1, n_classes)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "7eBPAgKsbIe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model function (following TF Estimator Template)\n",
        "def model_fn(features, labels, mode):\n",
        "    \n",
        "    # Build the neural network\n",
        "    # Because Dropout have different behavior at training and prediction time, we\n",
        "    # need to create 2 distinct computation graphs that still share the same weights.\n",
        "    logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
        "    logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
        "    \n",
        "    # Predictions\n",
        "    pred_classes = tf.argmax(logits_test, axis=1)\n",
        "    pred_probas = tf.nn.softmax(logits_test)\n",
        "    \n",
        "    # If prediction mode, early return\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
        "        \n",
        "    # Define loss and optimizer\n",
        "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
        "    \n",
        "    # Evaluate the accuracy of the model\n",
        "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
        "    \n",
        "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
        "    # the different ops for training, evaluating, ...\n",
        "    estim_specs = tf.estimator.EstimatorSpec(\n",
        "      mode=mode,\n",
        "      predictions=pred_classes,\n",
        "      loss=loss_op,\n",
        "      train_op=train_op,\n",
        "      eval_metric_ops={'accuracy': acc_op})\n",
        "\n",
        "    return estim_specs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fFxb5GQbIe7",
        "colab_type": "code",
        "outputId": "9cb4102f-e0c1-4fc0-a2fe-27215a180994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# Build the Estimator\n",
        "model = tf.estimator.Estimator(model_fn)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp4z3tb36e\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp4z3tb36e', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f35d62a3a20>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ258m2vbIfE",
        "colab_type": "code",
        "outputId": "ff38d34f-7027-4d94-c94b-b35fa02eda75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define the input function for training\n",
        "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x={'images': mnist.train.images}, y=mnist.train.labels,\n",
        "    batch_size=batch_size, num_epochs=10, shuffle=True)\n",
        "# Train the Model\n",
        "model.train(input_fn, steps=num_steps)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From <ipython-input-6-89f5665da817>:14: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-89f5665da817>:16: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-89f5665da817>:27: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-89f5665da817>:29: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py:882: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp4z3tb36e/model.ckpt.\n",
            "INFO:tensorflow:loss = 2.3100486, step = 0\n",
            "INFO:tensorflow:global_step/sec: 173.452\n",
            "INFO:tensorflow:loss = 0.10034053, step = 100 (0.580 sec)\n",
            "INFO:tensorflow:global_step/sec: 233.953\n",
            "INFO:tensorflow:loss = 0.058694325, step = 200 (0.427 sec)\n",
            "INFO:tensorflow:global_step/sec: 230.73\n",
            "INFO:tensorflow:loss = 0.07090832, step = 300 (0.434 sec)\n",
            "INFO:tensorflow:global_step/sec: 236.078\n",
            "INFO:tensorflow:loss = 0.08081372, step = 400 (0.423 sec)\n",
            "INFO:tensorflow:global_step/sec: 236.085\n",
            "INFO:tensorflow:loss = 0.10499014, step = 500 (0.425 sec)\n",
            "INFO:tensorflow:global_step/sec: 233.199\n",
            "INFO:tensorflow:loss = 0.036409613, step = 600 (0.428 sec)\n",
            "INFO:tensorflow:global_step/sec: 235.746\n",
            "INFO:tensorflow:loss = 0.048079498, step = 700 (0.424 sec)\n",
            "INFO:tensorflow:global_step/sec: 233.044\n",
            "INFO:tensorflow:loss = 0.018156417, step = 800 (0.430 sec)\n",
            "INFO:tensorflow:global_step/sec: 236.502\n",
            "INFO:tensorflow:loss = 0.024099452, step = 900 (0.422 sec)\n",
            "INFO:tensorflow:global_step/sec: 236.354\n",
            "INFO:tensorflow:loss = 0.059322886, step = 1000 (0.422 sec)\n",
            "INFO:tensorflow:global_step/sec: 236.383\n",
            "INFO:tensorflow:loss = 0.052924216, step = 1100 (0.423 sec)\n",
            "INFO:tensorflow:global_step/sec: 238.672\n",
            "INFO:tensorflow:loss = 0.030312112, step = 1200 (0.420 sec)\n",
            "INFO:tensorflow:global_step/sec: 229.772\n",
            "INFO:tensorflow:loss = 0.043564446, step = 1300 (0.436 sec)\n",
            "INFO:tensorflow:global_step/sec: 236.507\n",
            "INFO:tensorflow:loss = 0.03849245, step = 1400 (0.422 sec)\n",
            "INFO:tensorflow:global_step/sec: 228.472\n",
            "INFO:tensorflow:loss = 0.024064373, step = 1500 (0.439 sec)\n",
            "INFO:tensorflow:global_step/sec: 234.489\n",
            "INFO:tensorflow:loss = 0.042387836, step = 1600 (0.426 sec)\n",
            "INFO:tensorflow:global_step/sec: 233.403\n",
            "INFO:tensorflow:loss = 0.003016376, step = 1700 (0.427 sec)\n",
            "INFO:tensorflow:global_step/sec: 231.148\n",
            "INFO:tensorflow:loss = 0.014324472, step = 1800 (0.434 sec)\n",
            "INFO:tensorflow:global_step/sec: 238.361\n",
            "INFO:tensorflow:loss = 0.006510091, step = 1900 (0.419 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/tmp4z3tb36e/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.00776478.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7f35d62a3828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "059xrfeRaHhg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "2c0c1f45-b2bc-45fe-eacd-5ae12ea955db"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Apr  4 18:20:21 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P0    40W /  70W |    867MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIEukeUMbIfN",
        "colab_type": "code",
        "outputId": "2a79afbc-3799-4832-e3f4-3b052d57fa94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Evaluate the Model\n",
        "# Define the input function for evaluating\n",
        "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
        "    batch_size=batch_size, shuffle=False)\n",
        "# Use the Estimator 'evaluate' method\n",
        "model.evaluate(input_fn)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-04-04T18:20:23Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmp4z3tb36e/model.ckpt-2000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2020-04-04-18:20:24\n",
            "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.99, global_step = 2000, loss = 0.034245692\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: /tmp/tmp4z3tb36e/model.ckpt-2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.99, 'global_step': 2000, 'loss': 0.034245692}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsZFFlbRbIfW",
        "colab_type": "code",
        "outputId": "7c8780a5-4700-44dc-beaa-fea3a7b951b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Predict single images\n",
        "n_images = 4\n",
        "# Get images from test set\n",
        "test_images = mnist.test.images[:n_images]\n",
        "# Prepare the input data\n",
        "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x={'images': test_images}, shuffle=False)\n",
        "# Use the model to predict the images class\n",
        "preds = list(model.predict(input_fn))\n",
        "\n",
        "# Display\n",
        "for i in range(n_images):\n",
        "    plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n",
        "    plt.show()\n",
        "    print(\"Model prediction:\", preds[i])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmp4z3tb36e/model.ckpt-2000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCcIOJEHYgSQIO5DEvwEvYRv57rmVLgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model prediction: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model prediction: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model prediction: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyeFugDp7XnMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model prediction: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJp11STLawow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}